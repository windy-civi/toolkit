name: "OpenStates Scraper"
description: "Scrape legislative data via Docker and persist as artifacts"

inputs:
  state:
    description: "State abbreviation (e.g., id, il, tx, ny, or 'usa')"
    required: true
  github-token:
    description: "GitHub token for releases/artifacts"
    required: true
    default: "${{ github.token }}"
  use-scrape-cache:
    description: "Skip scraping and reuse the latest nightly artifact"
    required: false
    default: "false"
  commit-and-push:
    description: "Commit and push scraped files to git repository"
    required: false
    default: "false"
  force-update:
    description: "Force push even if upstream changed (use with caution)"
    required: false
    default: "false"
  branch:
    description: "Git branch to push to"
    required: false
    default: "main"
  api-keys:
    description: "JSON object with API keys for states that require them (e.g., DC_API_KEY, NEW_YORK_API_KEY, INDIANA_API_KEY, USER_AGENT). Note: Virginia uses csv_bills scraper and requires no API key."
    required: false
    default: "{}"

outputs:
  scrape-artifact-name:
    description: "Name of the uploaded scrape artifact"
    value: "scrape-snapshot-nightly"
  scrape-tarball-path:
    description: "Path to the scrape tarball"
    value: "${{ github.workspace }}/scrape-snapshot-nightly.tgz"
  manifest-path:
    description: "Path to the scrape manifest"
    value: "${{ github.workspace }}/scrape-manifest.json"

runs:
  using: "composite"
  steps:
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.13"

    - name: Prep working dirs
      shell: bash
      run: |
        set -euo pipefail
        mkdir -p "${RUNNER_TEMP}/_working/_data" "${RUNNER_TEMP}/_working/_cache"
        mkdir -p "${RUNNER_TEMP}/scrape-snapshot-nightly"
        echo "Working root: ${RUNNER_TEMP}"

    - name: Cache scrapes (best-effort)
      if: inputs.use-scrape-cache != 'true'
      uses: actions/cache@v4
      with:
        path: ${{ runner.temp }}/_working/_cache
        key: cache-scrapes-${{ inputs.state }}-${{ github.sha }}
        restore-keys: |
          cache-scrapes-${{ inputs.state }}-
          cache-scrapes-

    - name: Scrape data with Docker (resilient)
      if: inputs.use-scrape-cache != 'true'
      shell: bash
      working-directory: ${{ runner.temp }}
      env:
        DOCKER_IMAGE_TAG: latest
        API_KEYS_JSON: ${{ inputs.api-keys }}
      run: |
        set -euo pipefail
        # Persist image tag for later steps (manifest)
        echo "DOCKER_IMAGE_TAG=${DOCKER_IMAGE_TAG}" >> "${GITHUB_ENV}"

        # Use CA-specific script for California (MySQL-based)
        if [ "${{ inputs.state }}" = "ca" ]; then
          echo "üèõÔ∏è Using California-specific MySQL scraper..."
          bash "${{ github.action_path }}/scrape-ca.sh" "$(pwd)" "${{ github.workspace }}" "${DOCKER_IMAGE_TAG}" || true
        else
          bash "${{ github.action_path }}/scrape.sh" "${{ inputs.state }}" "${DOCKER_IMAGE_TAG}" "$(pwd)" "${{ github.workspace }}" "${API_KEYS_JSON}" || true
        fi

        # Check if tarball was created
        if [ -f "${{ github.workspace }}/scrape-snapshot-nightly.tgz" ]; then
          echo "SCRAPE_TARBALL=present" >> "${GITHUB_ENV}"
        else
          echo "SCRAPE_TARBALL=absent" >> "${GITHUB_ENV}"
        fi

    - name: Upload scraped artifact (rolling)
      if: inputs.use-scrape-cache != 'true' && env.SCRAPE_TARBALL == 'present'
      uses: actions/upload-artifact@v4
      with:
        name: scrape-snapshot-nightly
        path: ${{ github.workspace }}/scrape-snapshot-nightly.tgz
        retention-days: 30
        overwrite: true

    - name: Update nightly release (rolling)
      if: inputs.use-scrape-cache != 'true' && env.SCRAPE_TARBALL == 'present'
      uses: andelf/nightly-release@v1
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      with:
        tag_name: nightly
        name: "Nightly OpenStates scrape"
        prerelease: false
        files: |
          ${{ github.workspace }}/scrape-snapshot-nightly.tgz

    - name: Build archive manifest (immutable)
      if: env.SCRAPE_TARBALL == 'present'
      shell: bash
      run: |
        set -euo pipefail
        set -x
        SNAP_DATE=$(date -u +'%Y-%m-%d')
        echo "SNAP_DATE=${SNAP_DATE}" >> "${GITHUB_ENV}"

        TAR_PATH="${GITHUB_WORKSPACE}/scrape-snapshot-nightly.tgz"
        if [ ! -f "$TAR_PATH" ]; then
          echo "::error::Tarball not found at $TAR_PATH"
          exit 1
        fi

        # Count JSON files inside the tarball safely (don't abort on transient tar warnings)
        set +e
        COUNT=$(tar tzf "$TAR_PATH" 2>/dev/null | grep -E '\.json$' | wc -l | tr -d ' ')
        TAR_STATUS=$?
        set -e
        if [ "$TAR_STATUS" -ne 0 ]; then
          echo "::warning::Could not list tar contents, setting COUNT=0"
          COUNT=0
        fi

        # Cross-platform sha (ubuntu has sha256sum; fallback to shasum)
        if command -v sha256sum >/dev/null 2>&1; then
          SHA256=$(sha256sum "$TAR_PATH" | cut -d' ' -f1)
        else
          SHA256=$(shasum -a 256 "$TAR_PATH" | cut -d' ' -f1)
        fi

        cat > "${GITHUB_WORKSPACE}/scrape-manifest.json" <<JSON
        {
          "state": "${{ inputs.state }}",
          "date_utc": "${SNAP_DATE}",
          "files": ${COUNT},
          "sha256": "${SHA256}",
          "source_image": "openstates/scrapers:${DOCKER_IMAGE_TAG}"
        }
        JSON
        echo "üßæ Manifest written: scrape-manifest.json"

    - name: Publish immutable archive (date-stamped release)
      if: env.SCRAPE_TARBALL == 'present'
      uses: softprops/action-gh-release@v2
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      with:
        tag_name: archive-${{ inputs.state }}-${{ env.SNAP_DATE }}
        name: "Archive ${{ inputs.state }} ${{ env.SNAP_DATE }}"
        draft: false
        prerelease: false
        files: |
          ${{ github.workspace }}/scrape-snapshot-nightly.tgz
          ${{ github.workspace }}/scrape-manifest.json

    # Only download the nightly if we did NOT produce a fresh local tarball
    - name: Download nightly artifact (fallback)
      if: env.SCRAPE_TARBALL != 'present'
      uses: Xotl/cool-github-releases@v1
      with:
        mode: download
        tag_name: nightly
        assets: scrape-snapshot-nightly.tgz
        github_token: ${{ inputs.github-token }}

    - name: Ensure tarball exists
      if: env.SCRAPE_TARBALL != 'present'
      shell: bash
      run: |
        set -euo pipefail
        if [ ! -f "${GITHUB_WORKSPACE}/scrape-snapshot-nightly.tgz" ]; then
          echo "::error::No scrape tarball found (fresh scrape failed and nightly not available)."
          exit 1
        fi

    - name: Write scrape summary
      shell: bash
      run: |
        set -euo pipefail

        TAR_PATH="${GITHUB_WORKSPACE}/scrape-snapshot-nightly.tgz"
        SUMMARY_FILE="${GITHUB_WORKSPACE}/scrape-summary.json"

        # Determine source type
        if [ "${{ env.SCRAPE_TARBALL }}" = "present" ]; then
          SOURCE="üï∑Ô∏è Fresh scrape"
        else
          SOURCE="üì¶ Nightly fallback"
        fi

        # Count files in tarball
        if [ -f "$TAR_PATH" ]; then
          FILE_COUNT=$(tar tzf "$TAR_PATH" 2>/dev/null | grep -E '\.json$' | wc -l | tr -d ' ')
          STATUS="‚úÖ Success"
        else
          FILE_COUNT=0
          STATUS="‚ùå Failed"
        fi

        # Read parsed data from summary JSON if available
        if [ -f "$SUMMARY_FILE" ]; then
          # Main data objects
          BILL_COUNT=$(jq -r '.objects.bill // 0' "$SUMMARY_FILE")
          VOTE_EVENT_COUNT=$(jq -r '.objects.vote_event // 0' "$SUMMARY_FILE")
          EVENT_COUNT=$(jq -r '.objects.event // 0' "$SUMMARY_FILE")
          # Metadata objects
          JURISDICTION_COUNT=$(jq -r '.metadata.jurisdiction // 0' "$SUMMARY_FILE")
          ORG_COUNT=$(jq -r '.metadata.organization // 0' "$SUMMARY_FILE")
          # Other
          DURATION=$(jq -r '.duration // "unknown"' "$SUMMARY_FILE")
          ERROR_COUNT=$(jq -r '.error_count // 0' "$SUMMARY_FILE")
          ERRORS=$(jq -r '.errors | if length > 0 then .[] else empty end' "$SUMMARY_FILE" 2>/dev/null || echo "")
        else
          BILL_COUNT="N/A"
          VOTE_EVENT_COUNT="N/A"
          EVENT_COUNT="N/A"
          JURISDICTION_COUNT="N/A"
          ORG_COUNT="N/A"
          DURATION="N/A"
          ERROR_COUNT=0
          ERRORS=""
        fi

        # Calculate metadata total
        if [ "$JURISDICTION_COUNT" != "N/A" ] && [ "$ORG_COUNT" != "N/A" ]; then
          METADATA_TOTAL=$((JURISDICTION_COUNT + ORG_COUNT))
        else
          METADATA_TOTAL="N/A"
        fi

        # Write GitHub Actions summary
        echo "## üï∑Ô∏è Scrape Summary for ${{ inputs.state }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| üìú Bills | $BILL_COUNT |" >> $GITHUB_STEP_SUMMARY
        echo "| üó≥Ô∏è Vote Events | $VOTE_EVENT_COUNT |" >> $GITHUB_STEP_SUMMARY
        echo "| üìÖ Events | $EVENT_COUNT |" >> $GITHUB_STEP_SUMMARY
        echo "| üèõÔ∏è Metadata (jurisdiction + org) | $METADATA_TOTAL |" >> $GITHUB_STEP_SUMMARY
        echo "| üìÅ Total JSON Files | $FILE_COUNT |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| | |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| ‚è±Ô∏è Duration | $DURATION |" >> $GITHUB_STEP_SUMMARY
        echo "| üì¶ Source | $SOURCE |" >> $GITHUB_STEP_SUMMARY
        echo "| Status | $STATUS |" >> $GITHUB_STEP_SUMMARY

        # Show errors if any (only if scrape actually failed, or if there are critical errors)
        EXIT_CODE=$(jq -r '.exit_code // 0' "$SUMMARY_FILE" 2>/dev/null || echo "0")
        if [ -f "$SUMMARY_FILE" ] && [ "$ERROR_COUNT" -gt 0 ] && [ -n "$ERRORS" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "$EXIT_CODE" != "0" ]; then
            echo "### ‚ùå Errors Detected ($ERROR_COUNT) - Scrape Failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ö†Ô∏è Warnings/Errors Detected ($ERROR_COUNT) - Scrape Succeeded" >> $GITHUB_STEP_SUMMARY
          fi
          echo '```' >> $GITHUB_STEP_SUMMARY
          # Show up to 15 lines for tracebacks, 5 for other errors
          if echo "$ERRORS" | grep -q "Traceback"; then
            echo "$ERRORS" | head -15 >> $GITHUB_STEP_SUMMARY
          else
            echo "$ERRORS" | head -5 >> $GITHUB_STEP_SUMMARY
          fi
          echo '```' >> $GITHUB_STEP_SUMMARY
          if [ "$ERROR_COUNT" -gt 5 ]; then
            echo "_...and $((ERROR_COUNT - 5)) more. Check logs for details._" >> $GITHUB_STEP_SUMMARY
          fi
        fi

    - name: Fail action if scrape failed
      shell: bash
      run: |
        set -euo pipefail

        SUMMARY_FILE="${GITHUB_WORKSPACE}/scrape-summary.json"

        # Check if scrape failed by reading exit code from summary
        if [ -f "$SUMMARY_FILE" ]; then
          EXIT_CODE=$(jq -r '.exit_code // 0' "$SUMMARY_FILE")

          if [ "$EXIT_CODE" != "0" ]; then
            echo "‚ùå Scrape failed with exit code $EXIT_CODE"
            echo "::error::Scrape failed for ${{ inputs.state }}. Check the summary above for details."
            exit 1
          else
            echo "‚úÖ Scrape succeeded"
          fi
        else
          echo "‚ö†Ô∏è No summary file found, assuming success"
        fi

    - name: Extract files to workspace
      if: inputs.commit-and-push == 'true'
      shell: bash
      run: |
        set -euo pipefail
        TARBALL_PATH="${GITHUB_WORKSPACE}/scrape-snapshot-nightly.tgz"
        DATA_DIR="${GITHUB_WORKSPACE}/_data/${{ inputs.state }}"

        # Extract tarball if files don't exist in workspace (safety net)
        if [ -f "$TARBALL_PATH" ]; then
          if [ ! -d "$DATA_DIR" ] || [ -z "$(ls -A "$DATA_DIR" 2>/dev/null)" ]; then
            echo "üì¶ Extracting tarball to workspace (files not found)..."
            mkdir -p "$DATA_DIR"
            tar xzf "$TARBALL_PATH" -C "$DATA_DIR"
            EXTRACTED_COUNT=$(find "$DATA_DIR" -type f -name '*.json' 2>/dev/null | wc -l | tr -d ' ')
            echo "‚úÖ Extracted ${EXTRACTED_COUNT} files to _data/${{ inputs.state }}/"
          else
            EXISTING_COUNT=$(find "$DATA_DIR" -type f -name '*.json' 2>/dev/null | wc -l | tr -d ' ')
            echo "‚ÑπÔ∏è Files already exist in _data/${{ inputs.state }}/ (${EXISTING_COUNT} files)"
          fi
        fi

    - name: Commit and push scraped files
      if: inputs.commit-and-push == 'true'
      shell: bash
      run: |
        set -euo pipefail

        DATA_DIR="${GITHUB_WORKSPACE}/_data/${{ inputs.state }}"
        BRANCH="${{ inputs.branch }}"

        # Check if scraped files exist
        if [ ! -d "$DATA_DIR" ] || [ -z "$(ls -A "$DATA_DIR" 2>/dev/null)" ]; then
          echo "‚ö†Ô∏è No scraped files found in _data/${{ inputs.state }}/"
          exit 0
        fi

        # Check if we're in a git repository
        if [ ! -d "${GITHUB_WORKSPACE}/.git" ]; then
          echo "‚ö†Ô∏è Not in a git repository, skipping commit/push"
          exit 0
        fi

        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        # Add scraped files
        git add "_data/${{ inputs.state }}/"

        # Commit if there are changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "üï∑Ô∏è Scrape data for ${{ inputs.state }} - $(date -u +'%Y-%m-%d %H:%M:%S UTC')"

          # Pull latest changes before pushing
          echo "üì• Pulling latest changes..."
          if ! git pull --no-rebase origin "$BRANCH" 2>&1; then
            echo "‚ö†Ô∏è Merge conflict detected, resolving..."
            # In case of conflicts, keep our scraped files
            git checkout --ours "_data/${{ inputs.state }}/" 2>/dev/null || true
            git add "_data/${{ inputs.state }}/"
            git commit --no-edit -m "üîÑ Auto-merge: kept scraped data" || true
          fi

          # Push with retries
          PUSH_SUCCESS=false
          for i in 1 2 3; do
            if [ "${{ inputs.force-update }}" = "true" ]; then
              if git push --force-with-lease origin "$BRANCH"; then
                echo "‚úÖ Changes pushed successfully (attempt $i)"
                PUSH_SUCCESS=true
                break
              fi
            else
              if git push origin "$BRANCH"; then
                echo "‚úÖ Changes pushed successfully (attempt $i)"
                PUSH_SUCCESS=true
                break
              fi
            fi

            if [ $i -lt 3 ]; then
              echo "‚ö†Ô∏è Push failed (attempt $i), pulling and retrying..."
              git pull --no-rebase origin "$BRANCH" || true
              sleep 2
            fi
          done

          if [ "$PUSH_SUCCESS" = "false" ]; then
            echo "‚ùå Failed to push after 3 attempts"
            exit 1
          fi
        fi
